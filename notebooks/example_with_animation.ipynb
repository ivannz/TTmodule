{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TT-decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tt_hse16_slides](https://bayesgroup.github.io/team/arodomanov/tt_hse16_slides.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tensorising Neural Networks](https://arxiv.org/pdf/1509.06569.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfolding matrices into a tensor $\n",
    "    A \\in \\mathbb{R}^{n_0\\times \\ldots \\times n_{d-1}}\n",
    "$\n",
    "\n",
    "$$\n",
    "A_k = \\bigl(A_{i_{:k}, i_{k:}}\\bigr)_{i \\in \\prod_{j=0}^{d-1} [n_j]}\n",
    "    \\in \\mathbb{R}^{\n",
    "        [n_0 \\times \\ldots \\times n_{k-1}]\n",
    "        \\times [n_k \\times \\ldots \\times n_d]\n",
    "    }\n",
    "    \\,. $$\n",
    "\n",
    "where $n_{:k} = (n_j)_{j=0}^{k-1}$ and $n_{k:} = (n_j)_{j=k}^{d-1}$ -- zero-based like numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TT-format:\n",
    "\n",
    "$$\n",
    "A_{i} = \\sum_{\\alpha}\n",
    "    \\prod_{j=0}^{d-1} G_{\\alpha_j i_j \\alpha_{j+1}}\n",
    "    \\,, $$\n",
    "\n",
    "where $\n",
    "    G_{\\cdot i_j \\cdot} \\in \\mathbb{R}^{r_j \\times r_{j+1}}\n",
    "$ and $r_0 = r_d = 1$. The rank of the TT-decomposition is $r = \\max_{j=0}^d r_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Tensor-Train converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttmodule import tensor_to_tt, tt_to_tensor\n",
    "\n",
    "from ttmodule import matrix_to_tt, tt_to_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple, run-of-the-mill training loop.\n",
    "* imports from [`cplxmodule`](https://github.com/ivannz/cplxmodule.git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from cplxmodule.relevance import penalties\n",
    "from cplxmodule.utils.stats import sparsity\n",
    "\n",
    "def train_model(X, y, model, n_steps=20000, threshold=1.0,\n",
    "                klw=1e-3, verbose=False):\n",
    "    model.train()\n",
    "    optim = torch.optim.Adamax(model.parameters(), lr=2e-3)\n",
    "\n",
    "    losses, weights = [], []\n",
    "    with tqdm.tqdm(range(n_steps), disable=not verbose) as bar:\n",
    "        for i in bar:\n",
    "            optim.zero_grad()\n",
    "\n",
    "            y_pred = model(X)\n",
    "\n",
    "            mse = F.mse_loss(y_pred, y)\n",
    "            kl_d = sum(penalties(model))\n",
    "\n",
    "            loss = mse + klw * kl_d\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "            losses.append(float(loss))\n",
    "            bar.set_postfix_str(f\"{float(mse):.3e} {float(kl_d):.3e}\")\n",
    "            with torch.no_grad():\n",
    "                weights.append(model.weight.clone())\n",
    "        # end for\n",
    "    # end with\n",
    "    return model.eval(), losses, weights\n",
    "\n",
    "def test_model(X, y, model, threshold=1.0):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mse = F.mse_loss(model(X), y)\n",
    "        kl_d = sum(penalties(model))\n",
    "\n",
    "    f_sparsity = sparsity(model, threshold=threshold, hard=True)\n",
    "    print(f\"{f_sparsity:.1%} {mse.item():.3e} {float(kl_d):.3e}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttmodule import TTLinear\n",
    "\n",
    "from torch.nn import Linear\n",
    "from cplxmodule.relevance import LinearARD\n",
    "from cplxmodule.relevance import LinearL0ARD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the problem and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold, device_ = 3.0, \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple dataset: $\n",
    "    (x_i, y_i)_{i=1}^n \\in \\mathbb{R}^{d}\\times\\mathbb{R}^{p}\n",
    "$ and $y_i = E_{:p} x_i$ with $E_{:p} = (e_j)_{j=1}^p$ the diagonal\n",
    "projection matrix onto the first $p$ dimensions. We put $n\\leq p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "n_features, n_output = 250, 50\n",
    "\n",
    "X = torch.randn(10200, n_features)\n",
    "y = -X[:, :n_output].clone()\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X.to(device_), y.to(device_))\n",
    "\n",
    "train, test = dataset[:200], dataset[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A TT-linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful way of thinking about the TT-format of tensors is the following.\n",
    "If we assume the thelixcographic order of index traversl of the tensor $A$\n",
    "(`C`-order, or row-major) then\n",
    "\n",
    "$$\n",
    "A_\\mathbf{i}\n",
    "    = \\prod_{k=1}^d G^{(k)}_{i_k}\n",
    "    = \\sum_\\mathbf{\\alpha}\n",
    "        \\prod_{k=1}^d e_{\\alpha_{k-1}}^\\top G^{(k)}_{i_k} e_{\\alpha_k}\n",
    "    = \\sum_\\mathbf{\\alpha}\n",
    "        \\prod_{k=1}^d g^k_{\\alpha_{k-1} i_k \\alpha_k}\n",
    "    \\,, \\\\\n",
    "\\mathop{vec} A\n",
    "    = \\sum_\\mathbf{\\alpha}\n",
    "        g^1_{\\alpha_{0} \\alpha_1}\n",
    "        \\otimes g^2_{\\alpha_{1} \\alpha_2}\n",
    "        \\otimes \\cdots\n",
    "        \\otimes g^d_{\\alpha_{d-1} \\alpha_d}\n",
    "    \\,, $$\n",
    "\n",
    "with $\n",
    "    \\mathbf{i} = (i_k)_{k=1}^d\n",
    "$ running from $1$ to $\n",
    "    [n_1\\times \\ldots \\times n_d]\n",
    "$,\n",
    "$\\alpha$ running over $\\prod_{k=0}^d [r_k]$, $\\otimes$ being the Krnoecker product\n",
    "and `vec` taken in the lexicographic (row-major) order. The cores are $\n",
    "    G^{(k)}_{i_k} \\in \\mathbb{R}^{r_{k-1} \\times r_k}\n",
    "$\n",
    "$i_k \\in [n_k]$, and their `vec`-versions -- $\n",
    "    g^k_{\\alpha_{k-1} \\alpha_k} \\in \\mathbb{R}^{n_k}\n",
    "$\n",
    "for $\\alpha_{k-1} \\in [n_{k-1}]$ and $\\alpha_k \\in [n_k]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a matrix TT-decomposition with shapes $(n_k)_{k=1}^d$\n",
    "and $(m_k)_{k=1}^d$ we have:\n",
    "\n",
    "$$\n",
    "A = \\sum_\\mathbf{\\alpha}\n",
    "    B^1_{\\alpha_{0} \\alpha_1}\n",
    "    \\otimes \\cdots\n",
    "    \\otimes B^d_{\\alpha_{d-1} \\alpha_d}\n",
    "    \\,, $$\n",
    "\n",
    "with $\n",
    "    B^k_{\\alpha_{k-1} \\alpha_k} \\in \\mathbb{R}^{n_k\\times m_k}\n",
    "$ and\n",
    "$\n",
    "    B^k_{\\alpha_{k-1} \\alpha_k p q} = G^{(k)}_{\\alpha_{k-1} [p q] \\alpha_k}\n",
    "$, since each $i_k = [p q]$ is in fact a flattened index of the row-major\n",
    "flattened dimension $n_k\\times m_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix dimension factorization determines the block heirarchy of\n",
    "the matrix and thus is crucial to the properties and success of a linear\n",
    "layer with the weight in TT-format. If the linear layer in upstream,\n",
    "i.e. close to the inputs of the network, then the factorization and\n",
    "the induced heirarcy has semantic ties to the input features. In the\n",
    "mid-stream layers any particular heirarchy has less rationale, albeit\n",
    "it seems that the general-to-particular dimension factorization order\n",
    "is still preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed deep factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"detailed-deep-lo\"] = TTLinear(\n",
    "    [5, 5, 5, 2], [5, 5, 2, 1], rank=1, bias=False, reassemble=True)\n",
    "\n",
    "models[\"detailed-deep-hi\"] = TTLinear(\n",
    "    [5, 5, 5, 2], [5, 5, 2, 1], rank=5, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed shallow factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"detailed-shallow-lo\"] = TTLinear(\n",
    "    [25, 10], [25, 2], rank=1, bias=False, reassemble=True)\n",
    "\n",
    "models[\"detailed-shallow-hi\"] = TTLinear(\n",
    "    [25, 10], [25, 2], rank=5, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[\"detailed-lo\"] = TTLinear(\n",
    "#     [25, 5, 2], [5, 5, 2], rank=1, bias=False, reassemble=True)\n",
    "\n",
    "# models[\"detailed-lo\"] = TTLinear(\n",
    "#     [5, 5, 5, 1, 2], [5, 5, 2, 1, 1], rank=3, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"dotted\"] = TTLinear(\n",
    "#     [25, 10, 1], [5, 5, 2], rank=1, bias=False, reassemble=True)\n",
    "    [25, 5, 2], [5, 5, 2], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse deep factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one, with inverted hierarchy fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"coarse-deep-lo\"] = TTLinear(\n",
    "    [2, 5, 5, 5], [1, 2, 5, 5], rank=1, bias=False, reassemble=True)\n",
    "\n",
    "models[\"coarse-deep-hi\"] = TTLinear(\n",
    "    [2, 5, 5, 5], [1, 2, 5, 5], rank=5, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coarse shallow factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"coarse-shallow-lo\"] = TTLinear(\n",
    "    [10, 25], [5, 10], rank=1, bias=False, reassemble=True)\n",
    "\n",
    "models[\"coarse-shallow-hi\"] = TTLinear(\n",
    "    [10, 25], [5, 10], rank=5, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"striped\"] = TTLinear(\n",
    "    [5, 25, 2], [5, 5, 2], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LinearARD(n_features, n_output, bias=False)\n",
    "# model = LinearL0ARD(n_features, n_output, bias=False, group=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"blocked\"] = TTLinear(\n",
    "    [5, 25, 1, 2], [5, 5, 2, 1], rank=3, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [10, 25], [10, 5], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [5, 5, 10], [2, 5, 5], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [25, 10, 1], [2, 5, 5], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [25, 10], [2, 25], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [2, 5, 25], [2, 1, 25], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [2, 5, 25], [2, 5, 5], rank=4, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [5, 1, 25, 2], [1, 5, 2, 5], rank=3, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [25, 5, 2], [1, 25, 2], rank=1, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"test\"] = TTLinear(\n",
    "    [5, 25, 1, 2], [5, 5, 2, 1], rank=2, bias=False, reassemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, losses, weights = train_model(\n",
    "    *train, models[\"test\"], n_steps=2000,\n",
    "    threshold=threshold, klw=1e0, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_model(*test, model, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for core in model.cores:\n",
    "    plt.imshow(abs(core.detach()).numpy()[0, ..., 0].T,\n",
    "               cmap=plt.cm.bone, interpolation=None)\n",
    "\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... with not very simple setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def canvas_setup(figsize, **kwargs):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = GridSpec(1, 2, figure=fig, width_ratios=[7, 1])\n",
    "    ax_main = fig.add_subplot(gs[0])\n",
    "    ax_loss = fig.add_subplot(gs[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ax_main.imshow(abs(weights[0]).numpy(), cmap=plt.cm.bone)\n",
    "        ax_loss.semilogy(losses)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, (ax_main, ax_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canvas_clear(*axes):\n",
    "    \"\"\"Clear axis preserving its aesthetics.\"\"\"\n",
    "    for ax in axes:\n",
    "        props = ax.properties()\n",
    "        ax.clear()\n",
    "        ax.update({\n",
    "            k: props[k] for k in [\n",
    "                \"xticks\", \"yticks\", \"xlim\", \"ylim\", \"zorder\", \"alpha\"\n",
    "            ]\n",
    "        })\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_weight(n_epoch, *axes):\n",
    "    ax_main, ax_loss = canvas_clear(*axes)\n",
    "    \n",
    "    artists = []\n",
    "    with torch.no_grad():\n",
    "        artists.append(ax_main.imshow(\n",
    "            abs(weights[n_epoch]).numpy(),\n",
    "            cmap=plt.cm.bone,\n",
    "            interpolation=None\n",
    "        ))\n",
    "    artists.append(ax_main.set_title(f\"it. {n_epoch}\"))\n",
    "\n",
    "    artists.append(\n",
    "        ax_loss.semilogy(losses[:n_epoch + 1], lw=2, color=\"fuchsia\")\n",
    "    )\n",
    "    artists.append(\n",
    "        ax_loss.scatter([n_epoch + 1], [losses[n_epoch]],\n",
    "                        s=25, color=\"cyan\")\n",
    "    )\n",
    "    artists.append(\n",
    "        ax_loss.axvline(n_epoch + 1, c='cyan', lw=2, alpha=0.25, zorder=-10)\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        artist_ for artist_ in artists\n",
    "        if hasattr(artist_, \"set_animated\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interactive slider with ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "\n",
    "def int_slider(value, min, max, step):\n",
    "    return widgets.IntSlider(\n",
    "        value=value, min=min, max=max, step=step, continuous_update=False,\n",
    "        layout=widgets.Layout(min_width='500px', display='flex'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weight(n_epoch=0):\n",
    "    fig, axes = canvas_setup(figsize=(16, 3))\n",
    "    animate_weight(n_epoch, *axes)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "widgets.interact(plot_weight, n_epoch=int_slider(1000, 0, len(weights)-1, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "try:\n",
    "    FFMpegWriter = animation.writers['ffmpeg_file']\n",
    "    class PatchedFFMpegWriter(FFMpegWriter):\n",
    "        def setup(self, fig, outfile, *args, **kwargs):\n",
    "            dpi = kwargs.get(\"dpi\", getattr(self, \"dpi\", None))\n",
    "\n",
    "            frame_prefix = kwargs.get(\n",
    "                \"frame_prefix\", getattr(self, \"temp_prefix\", '_tmp'))\n",
    "\n",
    "            clear_temp = kwargs.get(\n",
    "                \"clear_temp\", getattr(self, \"clear_temp\", True))\n",
    "\n",
    "            super().setup(fig, outfile, clear_temp=clear_temp,\n",
    "                          frame_prefix=frame_prefix, dpi=dpi)\n",
    "\n",
    "except:\n",
    "    class PatchedFFMpegWriter(animation.AbstractMovieWriter):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tempfile\n",
    "\n",
    "dttm = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "fig, axes = canvas_setup(figsize=(16, 3))\n",
    "\n",
    "fps, n_frames = 15, len(weights)\n",
    "\n",
    "schedule = [\n",
    "    *range(0, 25, 1)\n",
    "] + [\n",
    "    *range(25, n_frames, 10)\n",
    "]\n",
    "\n",
    "shape_tag = model.extra_repr()\n",
    "outfile = os.path.join(\".\", f\"weight-{model.__class__.__name__}{shape_tag}-{dttm}.mp4\")\n",
    "\n",
    "# dump the intermediate frames into a temporary dir\n",
    "with tempfile.TemporaryDirectory() as tmp:\n",
    "    print(f\"temp dir at {tmp}\", flush=True)\n",
    "\n",
    "    writer = PatchedFFMpegWriter(fps=fps, bitrate=-1, metadata={})\n",
    "    writer.setup(fig, outfile, frame_prefix=os.path.join(tmp, f\"_frame_\"))\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, animate_weight, tqdm.tqdm_notebook(schedule, unit=\"frm\"),\n",
    "        interval=1, repeat_delay=None, blit=False, fargs=axes)\n",
    "    ani.save(outfile, writer=writer)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "print(outfile)\n",
    "Video(data=outfile, embed=True, width=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix-vector product in TT-format\n",
    "\n",
    "Suppose the TT representation of a matrix $W\\in \\mathbb{R}^{n\\times m}$\n",
    "with shapes $(n_k)_{k=1}^d$ and $(m_k)_{k=1}^d$ is given by $ \\prod_{k=1}^d\n",
    "G^{(k)}_{i_k j_k}$ with $\n",
    "    G^{(k)}_{i_k j_k} \\in \\mathbb{R}^{r_{k-1}\\times r_k}\n",
    "$ with $r_0 = r_d = 1$. Then for index $\n",
    "    \\alpha \\in \\prod_{k=1}^{d-1} [r_k]\n",
    "$ with $\\alpha_0 = \\alpha_d = 1$ we have:\n",
    "\n",
    "$$\n",
    "y_j = e_j^\\top W^\\top x\n",
    "    = \\sum_\\alpha \\sum_i \n",
    "          \\prod_{k=1}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k} x_i\n",
    "    = \\sum_{\\alpha_0, \\alpha_{1:}} \\sum_{i_{2:}} \n",
    "          \\prod_{k=2}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k}\n",
    "          \\sum_{i_1} g_{\\alpha_0 i_1 j_1 \\alpha_1} x_{i_1 i_{2:}}\n",
    "    = \\sum_{\\alpha_{1:}} \\sum_{i_{2:}} \n",
    "          \\prod_{k=2}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k}\n",
    "          \\sum_{\\alpha_0, i_1} g_{\\alpha_0 i_1 j_1 \\alpha_1} x_{i_1 i_{2:} \\alpha_0}\n",
    "    \\,,\\\\\n",
    "\\dots\n",
    "    = \\sum_{\\alpha_{1:}} \\sum_{i_{2:}} \n",
    "          \\prod_{k=2}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k} z_{i_{2:} j_1 \\alpha_1}\n",
    "    = \\sum_{\\alpha_{2:}} \\sum_{i_{3:}} \n",
    "          \\prod_{k=3}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k} z_{i_{3:} j_{:3} \\alpha_2}\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Rings\n",
    "\n",
    "See [Tensor Ring Decomposition](https://arxiv.org/abs/1606.05535). Essentially the same idea but\n",
    "with $t_0 = r_d \\geq 1$. Tensors in TT-format are a special case of TR-format:\n",
    "\n",
    "$$\n",
    "A_\\mathbf{i}\n",
    "    = \\mathop{Tr} \\prod_{k=1}^d G^{(k)}_{i_k}\n",
    "    = \\sum_{\\mathbf{\\alpha}\\colon \\alpha_0=\\alpha_d}\n",
    "        \\prod_{k=1}^d e_{\\alpha_{k-1}}^\\top G^{(k)}_{i_k} e_{\\alpha_k}\n",
    "    = \\sum_{\\mathbf{\\alpha}\\colon \\alpha_0=\\alpha_d}\n",
    "        \\prod_{k=1}^d g^k_{\\alpha_{k-1} i_k \\alpha_k}\n",
    "    \\,, $$\n",
    "\n",
    "where $\n",
    "    G^{(k)}_{i_k} \\in \\mathbb{R}^{r_j \\times r_{j+1}}\n",
    "$ and $r_0 = r_d$ and $\n",
    "    \\alpha \\in \\prod_{k=0}^d [n_k]\n",
    "$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks = [2, 3, 4, 5, 5]\n",
    "# shapes = [2, 3, 7, 4, 5], [3, 7, 7, 5, 2]\n",
    "\n",
    "ranks = [3, 2, 1, 5]\n",
    "shape = [2, 3, 7, 5], [3, 7, 1, 2]\n",
    "\n",
    "cores = [torch.randn(r0, n, m, r1, dtype=torch.double)\n",
    "         for r0, n, m, r1 in zip(ranks[-1:] + ranks[:-1], *shape, ranks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_to_tensor_zero(*cores):\n",
    "    # chip off the first core and contract the rest\n",
    "    rest = tt_to_tensor(*cores[1:], squeeze=False)\n",
    "\n",
    "    # contract with tensor_dot (reshape + einsum(\"i...j, j...i->...\") was slower)\n",
    "    return torch.tensordot(cores[0], rest, dims=[[0, -1], [-1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttmodule.tensor import tr_to_tensor\n",
    "\n",
    "res = %timeit -o -n 100 -r 25 tr_to_tensor_zero(*cores)\n",
    "\n",
    "timing = [res]\n",
    "for k in range(len(cores)):\n",
    "\n",
    "    res = %timeit -o -n 100 -r 25 tr_to_tensor(*cores, k=k)\n",
    "    timing.append(res)\n",
    "    print(f\">>> ({k}) {ranks[k]} {cores[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    W_{ij} = \\mathop{tr}\n",
    "        \\prod_{k=1}^d G^{(k)}_{i_k j_k}\n",
    "    \\,, \\\\\n",
    "    y_j = \\sum_i W_{ij} x_i\n",
    "        = \\sum_i \\mathop{tr} \\prod_{k=1}^d G^{(k)}_{i_k j_k} x_i\n",
    "        = \\mathop{tr} \\sum_i \\prod_{k=1}^d G^{(k)}_{i_k j_k} x_i\n",
    "        = \\mathop{tr} \\sum_{i_{1:}} \\sum_{i_1} \\prod_{k=1}^d G^{(k)}_{i_k j_k} x_i\n",
    "    \\,, \\\\\n",
    "    y_j = \\mathop{tr} \\sum_{i_{1:}}\n",
    "         \\prod_{k=2}^d G^{(k)}_{i_k j_k} \\sum_{i_1} G^{(1)}_{i_1 j_1} x_i\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttmodule.matrix import tr_to_matrix\n",
    "\n",
    "weight = tr_to_matrix(shape, *cores, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $\\alpha \\in \\prod_{k=1}^d [r_k]$ and $\\alpha_0 = \\alpha_d$\n",
    "and broadcasting $x_{i \\alpha_d} = x_{i}$\n",
    "$$\n",
    "    y = W^\\top x\n",
    "        = \\bigl( \\sum_i \\sum_\\alpha\n",
    "          \\prod_{k=1}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k} x_i \\bigr)_j\n",
    "        = \\bigl(\\sum_\\alpha \\sum_{i_{:d}} \n",
    "          \\prod_{k=1}^{d-1} g_{\\alpha_{k-1} i_k j_k \\alpha_k}\n",
    "               \\sum_{i_d} g_{\\alpha_{d-1} i_d j_d \\alpha_d} x_i \\bigr)_j\n",
    "    \\,, \\\\\n",
    "    y = W^\\top x\n",
    "        = \\bigl(\\sum_{\\alpha_d \\alpha_1} \\sum_{\\alpha_{2:d}} \\sum_{i_{2:}} \n",
    "          \\prod_{k=2}^d g_{\\alpha_{k-1} i_k j_k \\alpha_k}\n",
    "               \\sum_{i_1} g_{\\alpha_d i_1 j_1 \\alpha_1} x_i \\bigr)_j\n",
    "        = \\bigl(\\sum_{\\alpha_d \\alpha_1} \\sum_{i_{2:}} \n",
    "          Z_{\\alpha_1 i_{2:} j_{2:} \\alpha_d}\n",
    "               \\sum_{i_1} g_{\\alpha_d i_1 j_1 \\alpha_1} x_i \\bigr)_j\n",
    "    \\,, \\\\\n",
    "    y_j = e_j^\\top W^\\top x\n",
    "        = \\sum_{\\alpha_d} \\sum_i Z_{\\alpha_d i j \\alpha_d} x_i\n",
    "    \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttmv(shape, input, *cores):\n",
    "    *head, tail = input.shape\n",
    "    data = input.view(-1, *shape[0], 1)\n",
    "    for core in cores:\n",
    "        data = torch.tensordot(data, core, dims=[[1, -1], [1, 0]])\n",
    "\n",
    "    return data.reshape(*head, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(100, np.prod(shape[0])).double()\n",
    "\n",
    "reference = sum([\n",
    "    ttmv(shape, input, cores[ 0][[a], ...],\n",
    "         *cores[1:-1], cores[-1][..., [a]])\n",
    "    for a in range(ranks[-1])\n",
    "])\n",
    "\n",
    "assert torch.allclose(reference, torch.mm(input, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttmodule.matrix import invert\n",
    "\n",
    "\n",
    "def tr_vec(shape, input, *cores, k=0):\n",
    "    k = (len(cores) + k) if k < 0 else k\n",
    "    assert 0 <= k < len(cores)\n",
    "\n",
    "    *head, tail = input.shape\n",
    "    data = input.view(-1, *shape[0])\n",
    "\n",
    "    shuffle = list(range(1, data.dim()))\n",
    "    shuffle = 0, *shuffle[k:], *shuffle[:k]\n",
    "    data = data.permute(shuffle).unsqueeze(-1)\n",
    "\n",
    "    cores, output = cores[k:] + cores[:k], 0\n",
    "    for a in range(cores[0].shape[0]):\n",
    "        cyc = cores[ 0][[a], ...], *cores[1:-1], cores[-1][..., [a]]\n",
    "\n",
    "        interm = data.clone()\n",
    "        for core in cyc:\n",
    "            interm = torch.tensordot(interm, core, dims=[[1, -1], [1, 0]])\n",
    "        output += interm\n",
    "\n",
    "    return output.squeeze(-1).permute(invert(*shuffle)).reshape(*head, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(cores)):\n",
    "    assert torch.allclose(tr_vec(shape, input, *cores, k=k), reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*head, tail = input.shape\n",
    "data = input.view(-1, *shape[0], 1)\n",
    "for core in cores:\n",
    "    data = torch.tensordot(data, core, dims=[[1, -1], [1, 0]])\n",
    "data = data.sum(dim=-1).reshape(*head, -1)\n",
    "\n",
    "assert not torch.allclose(data, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed shape for TTLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks = [2, 3, 4, 5, 5]\n",
    "# shapes = [2, 3, 7, 4, 5], [3, 7, 7, 5, 2]\n",
    "\n",
    "ranks = [1, 3, 2, 5, 1]\n",
    "shape = [2, 3, 7, 5], [3, 7, 1, 2]\n",
    "\n",
    "cores = [torch.randn(r0, n, m, r1, dtype=torch.double)\n",
    "         for r0, n, m, r1 in zip(ranks[:-1], *shape, ranks[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_t = shape[1], shape[0]\n",
    "cores_t = [core.permute(0, 2, 1, 3) for core in cores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttmv_t(shape, input, *cores):\n",
    "    *head, tail = input.shape\n",
    "    data = input.view(-1, *shape[1], 1)\n",
    "    for core in cores:\n",
    "        data = torch.tensordot(data, core, dims=[[1, -1], [2, 0]])\n",
    "\n",
    "    return data.reshape(*head, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(ttmv_t(shape_t, input, *cores_t),\n",
    "                      ttmv(shape, input, *cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(tt_to_matrix(shape_t, *cores_t).t(),\n",
    "                      tt_to_matrix(shape, *cores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
